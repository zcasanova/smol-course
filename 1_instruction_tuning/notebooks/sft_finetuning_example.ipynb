{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Supervised Fine-Tuning with SFTTrainer\n",
                "\n",
                "This notebook demonstrates how to fine-tune the `HuggingFaceTB/SmolLM2-135M` model using the `SFTTrainer` from the `trl` library. We will follow best practices for preparing the dataset, configuring the trainer, and executing the training process."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import necessary libraries\n",
                "import torch\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
                "from datasets import load_dataset\n",
                "from trl import SFTConfig, SFTTrainer, setup_chat_format\n",
                "\n",
                "# Load the model and tokenizer\n",
                "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
                "model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_name).to(\"mps\")\n",
                "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)\n",
                "\n",
                "# Set up the chat format\n",
                "model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Dataset Preparation\n",
                "\n",
                "We will load a sample dataset and format it for training. The dataset should be structured with input-output pairs, where each input is a prompt and the output is the expected response from the model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2260/2260 [00:00<00:00, 217017.95 examples/s]\n",
                        "Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [00:00<00:00, 59292.25 examples/s]\n"
                    ]
                }
            ],
            "source": [
                "# Load a sample dataset\n",
                "from datasets import load_dataset\n",
                "\n",
                "ds = load_dataset(\"HuggingFaceTB/smoltalk\", \"everyday-conversations\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Configuring the SFTTrainer\n",
                "\n",
                "The `SFTTrainer` is configured with various parameters that control the training process. These include the number of training steps, batch size, learning rate, and evaluation strategy. Adjust these parameters based on your specific requirements and computational resources."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/Users/ben/code/smol-course/.venv/lib/python3.11/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
                        "  warnings.warn(\n",
                        "/Users/ben/code/smol-course/.venv/lib/python3.11/site-packages/transformers/training_args.py:2248: UserWarning: `use_mps_device` is deprecated and will be removed in version 5.0 of ðŸ¤— Transformers. `mps` device will be used by default if available similar to the way `cuda` device is used.Therefore, no action from user is required. \n",
                        "  warnings.warn(\n",
                        "/Users/ben/code/smol-course/.venv/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:309: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
                        "  warnings.warn(\n",
                        "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2260/2260 [00:00<00:00, 4997.11 examples/s]\n",
                        "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [00:00<00:00, 4926.97 examples/s]\n",
                        "max_steps is given, it will override any value given in num_train_epochs\n"
                    ]
                }
            ],
            "source": [
                "# Configure the SFTTrainer\n",
                "sft_config = SFTConfig(\n",
                "    output_dir=\"./sft_output\",\n",
                "    max_steps=1000,  # Adjust based on dataset size and desired training duration\n",
                "    per_device_train_batch_size=4,  # Set according to your GPU memory capacity\n",
                "    learning_rate=5e-5,  # Common starting point for fine-tuning\n",
                "    logging_steps=10,  # Frequency of logging training metrics\n",
                "    save_steps=100,  # Frequency of saving model checkpoints\n",
                "    evaluation_strategy=\"steps\",  # Evaluate the model at regular intervals\n",
                "    eval_steps=50,  # Frequency of evaluation\n",
                "    use_mps_device=True,\n",
                ")\n",
                "\n",
                "# Initialize the SFTTrainer\n",
                "trainer = SFTTrainer(\n",
                "    model=model, args=sft_config, train_dataset=ds[\"train\"], tokenizer=tokenizer, eval_dataset=ds[\"test\"]\n",
                ")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Training the Model\n",
                "\n",
                "With the trainer configured, we can now proceed to train the model. The training process will involve iterating over the dataset, computing the loss, and updating the model's parameters to minimize this loss."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train the model\n",
                "trainer.train()\n",
                "\n",
                "# Save the model\n",
                "# trainer.save_model(\"./sft_output\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusion\n",
                "\n",
                "This notebook provided a step-by-step guide to fine-tuning the `HuggingFaceTB/SmolLM2-135M` model using the `SFTTrainer`. By following these steps, you can adapt the model to perform specific tasks more effectively."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
