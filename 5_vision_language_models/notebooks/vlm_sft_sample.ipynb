{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning a VLM \n",
    "\n",
    "This notebook demonstrates how to fine-tune the `HuggingFaceTB/SmolVLM-Instruct` model using the `Trainer` from the `trl` library. The notebook cells run and will finetune the model. You can select your difficulty by trying out different datasets.\n",
    "\n",
    "<div style='background-color: lightblue; padding: 10px; border-radius: 5px; margin-bottom: 20px; color:black'>\n",
    "    <h2 style='margin: 0;color:blue'>Exercise: Fine-Tuning SmolVLM with SFTTrainer</h2>\n",
    "    <p>Take a dataset from the Hugging Face hub and finetune a model on it. </p> \n",
    "    <p><b>Difficulty Levels</b></p>\n",
    "    <p>üê¢ Use the `...` dataset with supervised method.</p>\n",
    "    <p>üêï Try preference optimization.</p>\n",
    "    <p>ü¶Å Try out the other datasets, other methods.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the requirements in Google Colab\n",
    "# !pip install transformers datasets trl huggingface_hub\n",
    "\n",
    "# Authenticate to Hugging Face\n",
    "from huggingface_hub import login\n",
    "login()\n",
    "\n",
    "# for convenience you can create an environment variable containing your hub token as HF_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "\n",
    "import torch\n",
    "import os\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq, BitsAndBytesConfig\n",
    "from transformers.image_utils import load_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True, \n",
    "                                         bnb_4bit_use_double_quant=True, \n",
    "                                         bnb_4bit_quant_type=\"nf4\", \n",
    "                                         bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "model_name = \"HuggingFaceTB/SmolVLM-Instruct\"\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ").to(device)\n",
    "processor = AutoProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM-Instruct\")\n",
    "\n",
    "# Set our name for the finetune to be saved &/ uploaded to\n",
    "finetune_name = \"SmolVLM-FT-MyDataset\"\n",
    "finetune_tags = [\"smol-course\", \"module_5\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "We will load a sample dataset and format it for training. The dataset should be structured with input-output pairs, where each input is a prompt and the output is the expected response from the model.\n",
    "\n",
    "**TRL will format input messages based on the model's chat templates.** They need to be represented as a list of dictionaries with the keys: `role` and `content`,."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# TODO: define your dataset and config using the path and name parameters\n",
    "dataset_name = \"HuggingFaceM4/ChartQA\"\n",
    "ds = load_dataset(path=dataset_name)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "sample = ds['train'][3]\n",
    "\n",
    "# Display the structure of a single sample\n",
    "sample['image_size'] = sample['image'].size,\n",
    "\n",
    "# Visualize the image and related metadata\n",
    "plt.imshow(sample['image'])\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Sample Chart Image\")\n",
    "plt.show()\n",
    "\n",
    "print(sample)\n",
    "\n",
    "# Preprocess the sample\n",
    "prompt = [{\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": sample[\"query\"]}]}]\n",
    "formatted_query = processor.apply_chat_template(prompt, tokenize=False)\n",
    "\n",
    "inputs = processor(\n",
    "    images=sample[\"image\"], \n",
    "    text=formatted_query, \n",
    "    return_tensors=\"pt\"\n",
    ").to(device)\n",
    "inputs = {key: val.to(device, dtype=torch.bfloat16) if val.dtype == torch.float else val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "# Generate predictions\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs,\n",
    "                             max_length=1600)\n",
    "\n",
    "# Decode the prediction\n",
    "prediction = processor.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Query: {sample['query']}\")\n",
    "print(f\"Expected Answer: {sample['label'][0]}\")\n",
    "print(f\"Model Prediction: {prediction[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system_message = \"\"\"You are a Vision Language Model specialized in interpreting visual data from chart images.\n",
    "# Your task is to analyze the provided chart image and respond to queries with concise answers, usually a single word, number, or short phrase.\n",
    "# The charts include a variety of types (e.g., line charts, bar charts) and contain colors, labels, and text.\n",
    "# Focus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.\"\"\"\n",
    "\n",
    "# def format_data(sample):\n",
    "#     return [\n",
    "#         {\n",
    "#             \"role\": \"system\",\n",
    "#             \"content\": [{\"type\": \"text\", \"text\": system_message}],\n",
    "#         },\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": [\n",
    "#                 {\n",
    "#                     \"type\": \"image\",\n",
    "#                     \"image\": sample[\"image\"],\n",
    "#                 },\n",
    "#                 {\n",
    "#                     \"type\": \"text\",\n",
    "#                     \"text\": sample[\"query\"],\n",
    "#                 },\n",
    "#             ],\n",
    "#         },\n",
    "#         {\n",
    "#             \"role\": \"assistant\",\n",
    "#             \"content\": [{\"type\": \"text\", \"text\": sample[\"label\"][0]}],\n",
    "#         },\n",
    "#     ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Configure LoRA\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=8,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Apply PEFT model adaptation\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the Trainer\n",
    "\n",
    "The `Trainer` is configured with various parameters that control the training process. These include the number of training steps, batch size, learning rate, and evaluation strategy. Adjust these parameters based on your specific requirements and computational resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    # System message template for the VLM\n",
    "    system_message = \"\"\"You are a Vision Language Model specialized in interpreting visual data from chart images.\n",
    "    Your task is to analyze the provided chart image and respond to queries with concise answers, usually a single word, number, or short phrase.\n",
    "    The charts include a variety of types (e.g., line charts, bar charts) and contain colors, labels, and text.\n",
    "    Focus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.\"\"\"\n",
    "    formatted_examples = []\n",
    "    for example in examples:\n",
    "    # Format examples to match the expected chat structure\n",
    "        formatted_examples.append(\n",
    "            [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": [{\"type\": \"text\", \"text\": system_message}],\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"image\",\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": example[\"query\"],\n",
    "                        },\n",
    "                    ],\n",
    "                },\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "\n",
    "    # text_inputs = [example[\"query\"] for example in examples]\n",
    "    text_inputs = processor.apply_chat_template(formatted_examples, tokenize=False)\n",
    "    \n",
    "    image_inputs = [example[\"image\"] for example in examples]  # Extract images\n",
    "\n",
    "    # Tokenize the texts and process the images\n",
    "    batch = processor(\n",
    "        text=text_inputs,\n",
    "        images=image_inputs,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    # Generate labels from the dataset\n",
    "    labels = processor.tokenizer(\n",
    "        [example[\"label\"][0] for example in examples],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True\n",
    "    ).input_ids\n",
    "\n",
    "    # Mask padding tokens in the labels\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "\n",
    "    # Add labels to the batch\n",
    "    batch[\"labels\"] = labels\n",
    "\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "# Configure the Trainer\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"sft_output\",  # Directory to save the model\n",
    "    num_train_epochs=3,  # Number of training epochs\n",
    "    per_device_train_batch_size=4,  # Batch size for training\n",
    "    per_device_eval_batch_size=4,  # Batch size for evaluation\n",
    "    gradient_accumulation_steps=8,  # Steps to accumulate gradients\n",
    "    gradient_checkpointing=True,  # Enable gradient checkpointing for memory efficiency\n",
    "    # Optimizer and scheduler settings\n",
    "    optim=\"adamw_torch_fused\",  # Optimizer type\n",
    "    learning_rate=2e-4,  # Learning rate for training\n",
    "    lr_scheduler_type=\"constant\",  # Type of learning rate scheduler\n",
    "    # Logging and evaluation\n",
    "    logging_steps=10,  # Steps interval for logging\n",
    "    eval_steps=10,  # Steps interval for evaluation\n",
    "    eval_strategy=\"steps\",  # Strategy for evaluation\n",
    "    save_strategy=\"steps\",  # Strategy for saving the model\n",
    "    save_steps=20,  # Steps interval for saving\n",
    "    metric_for_best_model=\"eval_loss\",  # Metric to evaluate the best model\n",
    "    greater_is_better=False,  # Whether higher metric values are better\n",
    "    load_best_model_at_end=True,  # Load the best model after training\n",
    "    # Mixed precision and gradient settings\n",
    "    bf16=True,  # Use bfloat16 precision\n",
    "    tf32=True,  # Use TensorFloat-32 precision\n",
    "    max_grad_norm=0.3,  # Maximum norm for gradient clipping\n",
    "    warmup_ratio=0.03,  # Ratio of total steps for warmup\n",
    "    # Hub and reporting\n",
    "    push_to_hub=True,  # Whether to push model to Hugging Face Hub\n",
    "    # Gradient checkpointing settings\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},  # Options for gradient checkpointing\n",
    "    # Dataset configuration\n",
    "    dataset_text_field=\"\",  # Text field in dataset\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},  # Additional dataset options\n",
    "    # max_seq_length=1024  # Maximum sequence length for input\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"test\"],\n",
    "    data_collator=collate_fn,\n",
    "    peft_config=peft_config,\n",
    "    # tokenizer=processor.tokenizer,\n",
    ")\n",
    "\n",
    "# TODO: ü¶Å üêï align the SFTTrainer params with your chosen dataset. For example, if you are using the `bigcode/the-stack-smol` dataset, you will need to choose the `content` column`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "With the trainer configured, we can now proceed to train the model. The training process will involve iterating over the dataset, computing the loss, and updating the model's parameters to minimize this loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(f\"./{finetune_name}\")\n",
    "\n",
    "# Save to the huggingface hub if login (HF_TOKEN is set)\n",
    "if os.getenv(\"HF_TOKEN\"):\n",
    "    trainer.push_to_hub(tags=finetune_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='background-color: lightblue; padding: 10px; border-radius: 5px; margin-bottom: 20px; color:black'>\n",
    "    <h2 style='margin: 0;color:blue'>Bonus Exercise: Generate with fine-tuned model</h2>\n",
    "    <p>üêï Use the fine-tuned to model generate a response, just like with the base example..</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíê You're done!\n",
    "\n",
    "This notebook provided a step-by-step guide to fine-tuning the `HuggingFaceTB/SmolVLM` model using the `Trainer`. By following these steps, you can adapt the model to perform specific tasks more effectively. If you want to carry on working on this course, here are steps you could try out:\n",
    "\n",
    "- Try this notebook on a harder difficulty\n",
    "- Review a colleagues PR\n",
    "- Improve the course material via an Issue or PR."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
