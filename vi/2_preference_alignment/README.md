# Tinh Ch·ªânh Theo S·ª± ∆Øu Ti√™n (Preference Alignment)

Trong ch∆∞∆°ng n√†y, b·∫°n s·∫Ω h·ªçc v·ªÅ c√°c k·ªπ thu·∫≠t tinh ch·ªânh m√¥ h√¨nh ng√¥n ng·ªØ theo s·ª± ∆∞u ti√™n c·ªßa con ng∆∞·ªùi. Trong khi *h·ªçc c√≥ gi√°m s√°t* gi√∫p m√¥ h√¨nh h·ªçc c√°c t√°c v·ª•, *tinh ch·ªânh theo s·ª± ∆∞u ti√™n* khuy·∫øn kh√≠ch ƒë·∫ßu ra ph√π h·ª£p v·ªõi k·ª≥ v·ªçng v√† gi√° tr·ªã c·ªßa con ng∆∞·ªùi.

## T·ªïng Quan

C√°c ph∆∞∆°ng ph√°p *tinh ch·ªânh theo s·ª± ∆∞u ti√™n* th∆∞·ªùng bao g·ªìm 2 giai ƒëo·∫°n:

1. B·∫Øt ƒë·∫ßu b·∫±ng qu√° tr√¨nh *h·ªçc c√≥ gi√°m s√°t* (SFT) ƒë·ªÉ th√≠ch ·ª©ng m√¥ h√¨nh v·ªõi c√°c lƒ©nh v·ª±c c·ª• th·ªÉ
2. Sau ƒë√≥, tinh ch·ªânh m√¥ h√¨nh theo s·ª± ∆∞u ti√™n (nh∆∞ RLHF ho·∫∑c DPO) ƒë·ªÉ c·∫£i thi·ªán ch·∫•t l∆∞·ª£ng ph·∫£n h·ªìi

C√°c ph∆∞∆°ng ph√°p thay th·∫ø nh∆∞ ORPO k·∫øt h·ª£p c·∫£ *tinh ch·ªânh theo ch·ªâ th·ªã* v√† *tinh ch·ªânh theo s·ª± ∆∞u ti√™n* th√†nh 1 giai ƒëo·∫°n tinh ch·ªânh duy nh·∫•t. ·ªû ƒë√¢y, ch√∫ng ta s·∫Ω t·∫≠p trung v√†o c√°c thu·∫≠t to√°n DPO v√† ORPO.

N·∫øu b·∫°n mu·ªën t√¨m hi·ªÉu th√™m v·ªÅ c√°c k·ªπ thu·∫≠t tinh ch·ªânh kh√°c, b·∫°n c√≥ th·ªÉ ƒë·ªçc th√™m t·∫°i [Argilla Blog](https://argilla.io/blog/mantisnlp-rlhf-part-8).

### 1Ô∏è‚É£ T·ªëi ∆Øu H√≥a ∆Øu Ti√™n Tr·ª±c Ti·∫øp (Direct Preference Optimization - DPO)

Ph∆∞∆°ng ph√°p n√†y ƒë∆°n gi·∫£n h√≥a qu√° tr√¨nh *tinh ch·ªânh theo ch·ªâ th·ªã* b·∫±ng c√°ch t·ªëi ∆∞u h√≥a tr·ª±c ti·∫øp m√¥ h√¨nh s·ª≠ d·ª•ng d·ªØ li·ªáu ∆∞u ti√™n (preference data). Ph∆∞∆°ng ph√°p n√†y lo·∫°i b·ªè nhu c·∫ßu v·ªÅ c√°c *M√¥ h√¨nh th∆∞·ªüng ph·∫°t* (Reward model) ri√™ng bi·ªát v√† *H·ªçc tƒÉng c∆∞·ªùng* ph·ª©c t·∫°p, gi√∫p qu√° tr√¨nh ·ªïn ƒë·ªãnh v√† hi·ªáu qu·∫£ h∆°n so v·ªõi H·ªçc tƒÉng c∆∞·ªùng t·ª´ ph·∫£n h·ªìi c·ªßa con ng∆∞·ªùi (RLHF) truy·ªÅn th·ªëng. ƒê·ªÉ bi·∫øt th√™m chi ti·∫øt, b·∫°n c√≥ th·ªÉ tham kh·∫£o t√†i li·ªáu [*t·ªëi ∆∞u h√≥a ∆∞u ti√™n tr·ª±c ti·∫øp* (DPO)](./dpo.md).

### 2Ô∏è‚É£ T·ªëi ∆Øu H√≥a ∆Øu Ti√™n Theo T·ª∑ L·ªá Odds (Odds Ratio Preference Optimization - ORPO)

ORPO gi·ªõi thi·ªáu m·ªôt ph∆∞∆°ng ph√°p k·∫øt h·ª£p c·∫£ 2 giai ƒëo·∫°n *tinh ch·ªânh theo ch·ªâ th·ªã* v√† *tinh ch·ªânh theo s·ª± ∆∞u ti√™n* v√†o trong 1 giai ƒëo·∫°n tinh ch·ªânh duy nh·∫•t. Ph∆∞∆°ng ph√°p n√†y ƒëi·ªÅu ch·ªânh m·ª•c ti√™u ti√™u chu·∫©n c·ªßa m√¥ h√¨nh ng√¥n ng·ªØ b·∫±ng c√°ch k·∫øt h·ª£p *negative log-likelihood loss* v·ªõi m·ªôt * t·ª∑ l·ªá odds* ·ªü c·∫•p ƒë·ªô *token*. V√¨ v·∫≠y, ORPO t·∫°o ra 1 qu√° tr√¨nh tinh ch·ªânh th·ªëng nh·∫•t v·ªõi ki·∫øn tr√∫c kh√¥ng c·∫ßn m√¥ h√¨nh th∆∞·ªüng ph·∫°t v√† c·∫£i thi·ªán ƒë√°ng k·ªÉ hi·ªáu qu·∫£ t√≠nh to√°n. ORPO ƒë√£ cho th·∫•y k·∫øt qu·∫£ ·∫•n t∆∞·ª£ng tr√™n nhi·ªÅu benchmark, th·ªÉ hi·ªán hi·ªáu su·∫•t t·ªët h∆°n tr√™n AlpacaEval so v·ªõi c√°c ph∆∞∆°ng ph√°p truy·ªÅn th·ªëng. ƒê·ªÉ bi·∫øt th√™m chi ti·∫øt, b·∫°n c√≥ th·ªÉ tham kh·∫£o t√†i li·ªáu [t·ªëi ∆∞u h√≥a ∆∞u ti√™n theo t·ª∑ l·ªá odds (ORPO)](./orpo.md).

## B√†i T·∫≠p

| Ti√™u ƒë·ªÅ | M√¥ t·∫£ | B√†i t·∫≠p | ƒê∆∞·ªùng d·∫´n | Colab |
|-------|-------------|----------|------|-------|
| Tinh ch·ªânh theo DPO | H·ªçc c√°ch tinh ch·ªânh m√¥ h√¨nh b·∫±ng ph∆∞∆°ng ph√°p t·ªëi ∆∞u h√≥a ∆∞u ti√™n tr·ª±c ti·∫øp | üê¢ Tinh ch·ªânh m√¥ h√¨nh s·ª≠ d·ª•ng b·ªô d·ªØ li·ªáu HH-RLHF <br>üêï S·ª≠ d·ª•ng t·∫≠p d·ªØ li·ªáu c·ªßa ri√™ng b·∫°n<br>ü¶Å Th·ª≠ nghi·ªám v·ªõi c√°c t·∫≠p d·ªØ li·ªáu v√† k√≠ch th∆∞·ªõc m√¥ h√¨nh kh√°c nhau | [Notebook](./notebooks/dpo_finetuning_example.ipynb) | <a target="_blank" href="https://colab.research.google.com/github/huggingface/smol-course/blob/main/2_preference_alignment/notebooks/dpo_finetuning_example.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="S·ª≠ d·ª•ng Colab"/></a> |
| Tinh ch·ªânh theo ORPO | H·ªçc c√°ch tinh ch·ªânh m√¥ h√¨nh b·∫±ng ph∆∞∆°ng ph√°p t·ªëi ∆∞u h√≥a ∆∞u ti√™n theo t·ª∑ l·ªá odds | üê¢ Hu·∫•n luy·ªán m√¥ h√¨nh s·ª≠ d·ª•ng b·ªô d·ªØ li·ªáu ch·ªâ th·ªã (instruction) v√† d·ªØ li·ªáu ∆∞u ti√™n (preference)<br>üêï Th·ª≠ nghi·ªám v·ªõi c√°c tr·ªçng s·ªë loss kh√°c nhau<br>ü¶Å So s√°nh k·∫øt qu·∫£ ORPO v·ªõi DPO | [Notebook](./notebooks/orpo_finetuning_example.ipynb) | <a target="_blank" href="https://colab.research.google.com/github/huggingface/smol-course/blob/main/2_preference_alignment/notebooks/orpo_finetuning_example.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="S·ª≠ d·ª•ng Colab"/></a> |


## Resources

- [T√†i li·ªáu th∆∞ vi·ªán TRL](https://huggingface.co/docs/trl/index) - T√†i li·ªáu cho th∆∞ vi·ªán Transformers Reinforcement Learning (TRL), tri·ªÉn khai nhi·ªÅu k·ªπ thu·∫≠t cƒÉn ch·ªânh bao g·ªìm DPO v√† ORPO.
- [B√†i b√°o nghi√™n c·ª©u DPO](https://arxiv.org/abs/2305.18290) - b√†i nghi√™n c·ª©u g·ªëc gi·ªõi thi·ªáu *t·ªëi ∆∞u h√≥a ∆∞u ti√™n tr·ª±c ti·∫øp* nh∆∞ m·ªôt gi·∫£i ph√°p thay th·∫ø ƒë∆°n gi·∫£n h∆°n cho RLHF.
- [B√†i b√°o nghi√™n c·ª©u ORPO](https://arxiv.org/abs/2403.07691) - Gi·ªõi thi·ªáu Odds Ratio Preference Optimization, m·ªôt ph∆∞∆°ng ph√°p m·ªõi k·∫øt h·ª£p *tinh ch·ªânh theo ch·ªâ th·ªã* v√† *tinh ch·ªânh theo s·ª± ∆∞u ti√™n* th√†nh 1
- [B√†i h∆∞·ªõng d·∫´n c·ªßa Argilla](https://argilla.io/blog/mantisnlp-rlhf-part-8/) - H∆∞·ªõng d·∫´n gi·∫£i th√≠ch c√°c k·ªπ thu·∫≠t cƒÉn ch·ªânh kh√°c nhau bao g·ªìm RLHF, DPO v√† c√°ch tri·ªÉn khai th·ª±c t·∫ø.
- [Blog v·ªÅ DPO](https://huggingface.co/blog/dpo-trl) - H∆∞·ªõng d·∫´n th·ª±c h√†nh v·ªÅ tri·ªÉn khai DPO s·ª≠ d·ª•ng th∆∞ vi·ªán TRL v·ªõi c√°c v√≠ d·ª• code v√† ph∆∞∆°ng ph√°p t·ªët nh·∫•t.
- [Code m·∫´u cho DPO trong th∆∞ vi√™n TRL](https://github.com/huggingface/trl/blob/main/examples/scripts/dpo.py) - Code m·∫´u v·ªÅ c√°ch tri·ªÉn khai tinh ch·ªânh DPO s·ª≠ d·ª•ng th∆∞ vi·ªán TRL.
- [Code m·∫´u cho ORPD trong th∆∞ vi√™n TRL](https://github.com/huggingface/trl/blob/main/examples/scripts/orpo.py) - Code m·∫´u c·ªßa tinh ch·ªânh ORPO s·ª≠ d·ª•ng th∆∞ vi·ªán TRL v·ªõi c√°c t√πy ch·ªçn c·∫•u h√¨nh chi ti·∫øt.
- [Hugging Face Alignment Handbook](https://github.com/huggingface/alignment-handbook) - H∆∞·ªõng d·∫´n v√† codebase cho vi·ªác tinh ch·ªânh m√¥ h√¨nh ng√¥n ng·ªØ s·ª≠ d·ª•ng c√°c k·ªπ thu·∫≠t kh√°c nhau bao g·ªìm SFT, DPO v√† RLHF.
