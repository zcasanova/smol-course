# 지도 학습 기반 미세 조정(Supervised Fine-Tuning)

지도 학습 기반 미세 조정(SFT)은 사전 학습된 언어 모델을 특정 작업이나 도메인에 맞게 조정하는 데 핵심적인 과정입니다. 사전 학습된 모델의 능력은 일반적으로 뛰어나지만, 특정 사용 사례에서 우수한 성능을 발휘하려면 맞춤화가 필요할 때가 많습니다. SFT는 사람이 검증한 예제를 포함하여 신중하게 선별된 데이터셋으로 모델을 추가 학습시켜 이러한 격차를 해소합니다.

## SFT 이해하기

SFT의 핵심은 레이블이 있는 토큰을 통해 사전 학습된 모델이 특정 작업을 수행하도록 학습시키는 것입니다. 이 과정에서 원하는 입력-출력 동작 예제를 모델에 많이 보여줌으로써, 해당 사용 사례에 특화된 패턴을 학습하도록 합니다.

SFT가 효과적인 이유는 사전 학습 과정에서 습득한 기초 지식을 활용하면서도 사용자의 특정 요구에 맞게 모델의 동작을 조정할 수 있기 때문입니다.

## SFT를 언제 사용해야 하나요?

SFT 사용 여부는 주로 모델이 현재 가지고 있는 능력과 특정 요구 사항 간의 차이에 따라 결정됩니다. SFT는 특히 모델 출력에 대한 정교한 제어가 필요하거나 전문적인 도메인에서 작업할 때 매우 유용합니다.

예를 들어, 고객 서비스 애플리케이션을 개발하는 경우 모델이 회사 지침을 일관되게 따르고 기술적인 질문을 표준화된 방식으로 처리하도록 만들고 싶을 것입니다. 마찬가지로 의료나 법률 애플리케이션에서는 정확성과 도메인 특화 용어를 사용하는 것이 중요합니다. SFT는 이러한 경우에서 모델의 응답이 전문적인 표준을 따르고 모델 응답을 도메인 전문 지식에 맞게 조정하는 데 도움을 줄 수 있습니다.

## 미세 조정 과정

지도 학습 기반 미세 조정은 특정 태스크에 맞춘 데이터셋으로 모델 가중치를 학습시키는 과정을 포함합니다.

먼저, 목표 태스크를 대표할 수 있는 데이터셋을 준비하거나 선택해야 합니다. 이 데이터셋은 모델이 마주하게 될 다양한 시나리오를 포괄할 수 있는 예제를 포함하고 있어야 합니다. 데이터 품질이 매우 중요하며 각 예제는 모델이 생성하길 원하는 출력 유형을 명확히 보여줘야 합니다. 이후 Hugging Face의 `transformers` 및 `trl`과 같은 프레임워크를 사용하여 데이터셋을 기반으로 모델을 학습시키는 실제 미세 조정 단계가 이어집니다.

이 과정 전반에서 지속적인 평가가 필수적입니다. 모델이 원하는 동작을 학습하면서도 일반적인 능력을 잃지 않도록 검증 데이터셋을 사용해 성능을 모니터링해야 합니다. 모델을 평가하는 방법에 대해서는 [모듈 4](../4_evaluation)에서 다룰 예정입니다.

## 선호도 조정에서 SFT의 역할

SFT는 언어 모델을 인간의 선호도에 맞추는 데 기본적인 역할을 합니다. 인간 피드백을 통한 강화 학습(Reinforcement Learning from Human Feedback, RLHF) 및 직접 선호도 최적화(Direct Preference Optimization, DPO)와 같은 기술은 SFT를 사용하여 기본적인 수준으로 태스크를 이해하게 한 후 모델의 응답을 원하는 결과에 맞게 조정합니다. 사전 학습된 모델은 일반적인 언어 능력을 가지고 있음에도 불구하고 항상 사람의 선호도와 일치하는 결과를 생성하지 못할 수 있습니다. SFT는 도메인별 데이터와 지침을 도입하여 이러한 격차를 해소함으로써 모델이 인간의 기대에 부합하는 응답을 생성할 수 있도록 합니다.

## TRL을 활용한 SFT

지도 학습 기반 미세 조정에서 중요한 소프트웨어 패키지 중 하나는 Transformer 강화 학습(Transformer Reinforcement Learning, TRL)입니다. TRL은 강화 학습(Reinforcement Learning, RL)을 사용하여 트랜스포머 언어 모델을 학습시키기 위한 도구입니다.

Hugging Face Transformers 라이브러리를 기반으로 구축된 TRL은 사용자가 사전 학습된 언어 모델을 직접 로드할 수 있게 하며, 대부분의 디코더 및 인코더-디코더 아키텍처를 지원합니다. 이 라이브러리는 언어 모델링에 사용되는 주요 RL 프로세스를 지원하고 지도 학습 기반 미세 조정(SFT), 보상 모델링(Reward Modeling, RM), 근접 정책 최적화(Proximal Policy Optimization, PPO), 직접 선호 최적화(Direct Preference Optimization, DPO)와 같은 과정을 용이하게 만듭니다.

이 레포지토리의 여러 모듈에서 TRL을 활용할 예정입니다.

# 다음 단계

다음 튜토리얼을 통해 TRL을 활용한 SFT 실습 경험을 쌓아 보세요: 

⏭️ [Chat Templates 튜토리얼](./notebooks/chat_templates_example.ipynb)

⏭️ [Supervised Fine-Tuning 튜토리얼](./notebooks/supervised_fine_tuning_tutorial.ipynb)